
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>spynnaker.pyNN.models.neuron.synapse_dynamics.synapse_dynamics_stdp &#8212; sPyNNaker 6.0.1 documentation</title>
    <link rel="stylesheet" type="text/css" href="../../../../../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../../../../../_static/sphinxdoc.css" />
    <script data-url_root="../../../../../../" id="documentation_options" src="../../../../../../_static/documentation_options.js"></script>
    <script src="../../../../../../_static/jquery.js"></script>
    <script src="../../../../../../_static/underscore.js"></script>
    <script src="../../../../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../../../../../_static/doctools.js"></script>
    <link rel="index" title="Index" href="../../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../../search.html" /> 
  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../../../../../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="../../../../../../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="nav-item nav-item-0"><a href="../../../../../../index.html">sPyNNaker 6.0.1 documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="../../../../../index.html" >Module code</a> &#187;</li>
          <li class="nav-item nav-item-2"><a href="../../../../pyNN.html" accesskey="U">spynnaker.pyNN</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">spynnaker.pyNN.models.neuron.synapse_dynamics.synapse_dynamics_stdp</a></li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <h1>Source code for spynnaker.pyNN.models.neuron.synapse_dynamics.synapse_dynamics_stdp</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright (c) 2017-2019 The University of Manchester</span>
<span class="c1">#</span>
<span class="c1"># This program is free software: you can redistribute it and/or modify</span>
<span class="c1"># it under the terms of the GNU General Public License as published by</span>
<span class="c1"># the Free Software Foundation, either version 3 of the License, or</span>
<span class="c1"># (at your option) any later version.</span>
<span class="c1">#</span>
<span class="c1"># This program is distributed in the hope that it will be useful,</span>
<span class="c1"># but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="c1"># MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="c1"># GNU General Public License for more details.</span>
<span class="c1">#</span>
<span class="c1"># You should have received a copy of the GNU General Public License</span>
<span class="c1"># along with this program.  If not, see &lt;http://www.gnu.org/licenses/&gt;.</span>

<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">numpy</span>
<span class="kn">from</span> <span class="nn">pyNN.standardmodels.synapses</span> <span class="kn">import</span> <span class="n">StaticSynapse</span>
<span class="kn">from</span> <span class="nn">spinn_utilities.overrides</span> <span class="kn">import</span> <span class="n">overrides</span>
<span class="kn">from</span> <span class="nn">spinn_front_end_common.abstract_models</span> <span class="kn">import</span> <span class="n">AbstractChangableAfterRun</span>
<span class="kn">from</span> <span class="nn">spinn_front_end_common.utilities.constants</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">BYTES_PER_WORD</span><span class="p">,</span> <span class="n">BYTES_PER_SHORT</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">spinn_front_end_common.utilities.globals_variables</span> <span class="kn">import</span> <span class="n">get_simulator</span>
<span class="kn">from</span> <span class="nn">spynnaker.pyNN.models.abstract_models</span> <span class="kn">import</span> <span class="n">AbstractSettable</span>
<span class="kn">from</span> <span class="nn">spynnaker.pyNN.exceptions</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">InvalidParameterType</span><span class="p">,</span> <span class="n">SynapticConfigurationException</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">spynnaker.pyNN.utilities.utility_calls</span> <span class="kn">import</span> <span class="n">get_n_bits</span>
<span class="kn">from</span> <span class="nn">.abstract_plastic_synapse_dynamics</span> <span class="kn">import</span> <span class="n">AbstractPlasticSynapseDynamics</span>
<span class="kn">from</span> <span class="nn">.abstract_synapse_dynamics_structural</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">AbstractSynapseDynamicsStructural</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">.abstract_generate_on_machine</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">AbstractGenerateOnMachine</span><span class="p">,</span> <span class="n">MatrixGeneratorID</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">.synapse_dynamics_neuromodulation</span> <span class="kn">import</span> <span class="n">SynapseDynamicsNeuromodulation</span>

<span class="c1"># How large are the time-stamps stored with each event</span>
<span class="n">TIME_STAMP_BYTES</span> <span class="o">=</span> <span class="n">BYTES_PER_WORD</span>

<span class="c1"># The targets of neuromodulation</span>
<span class="n">NEUROMODULATION_TARGETS</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;reward&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="s2">&quot;punishment&quot;</span><span class="p">:</span> <span class="mi">1</span>
<span class="p">}</span>


<span class="k">class</span> <span class="nc">SynapseDynamicsSTDP</span><span class="p">(</span>
        <span class="n">AbstractPlasticSynapseDynamics</span><span class="p">,</span> <span class="n">AbstractSettable</span><span class="p">,</span>
        <span class="n">AbstractChangableAfterRun</span><span class="p">,</span> <span class="n">AbstractGenerateOnMachine</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot; The dynamics of a synapse that changes over time using a \</span>
<span class="sd">        Spike Timing Dependent Plasticity (STDP) rule.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="vm">__slots__</span> <span class="o">=</span> <span class="p">[</span>
        <span class="c1"># Flag: whether there is state in this class that is not reflected on</span>
        <span class="c1"># the SpiNNaker system</span>
        <span class="s2">&quot;__change_requires_mapping&quot;</span><span class="p">,</span>
        <span class="c1"># Fraction of delay that is dendritic (instead of axonal or synaptic)</span>
        <span class="s2">&quot;__dendritic_delay_fraction&quot;</span><span class="p">,</span>
        <span class="c1"># timing dependence to use for the STDP rule</span>
        <span class="s2">&quot;__timing_dependence&quot;</span><span class="p">,</span>
        <span class="c1"># weight dependence to use for the STDP rule</span>
        <span class="s2">&quot;__weight_dependence&quot;</span><span class="p">,</span>
        <span class="c1"># The neuromodulation instance if enabled</span>
        <span class="s2">&quot;__neuromodulation&quot;</span><span class="p">,</span>
        <span class="c1"># padding to add to a synaptic row for synaptic rewiring</span>
        <span class="s2">&quot;__pad_to_length&quot;</span><span class="p">,</span>
        <span class="c1"># Weight of connections formed by connector</span>
        <span class="s2">&quot;__weight&quot;</span><span class="p">,</span>
        <span class="c1"># Delay of connections formed by connector</span>
        <span class="s2">&quot;__delay&quot;</span><span class="p">,</span>
        <span class="c1"># Whether to use back-propagation delay or not</span>
        <span class="s2">&quot;__backprop_delay&quot;</span><span class="p">]</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span> <span class="n">timing_dependence</span><span class="p">,</span> <span class="n">weight_dependence</span><span class="p">,</span>
            <span class="n">voltage_dependence</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dendritic_delay_fraction</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
            <span class="n">weight</span><span class="o">=</span><span class="n">StaticSynapse</span><span class="o">.</span><span class="n">default_parameters</span><span class="p">[</span><span class="s1">&#39;weight&#39;</span><span class="p">],</span>
            <span class="n">delay</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">pad_to_length</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">backprop_delay</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        :param AbstractTimingDependence timing_dependence:</span>
<span class="sd">        :param AbstractWeightDependence weight_dependence:</span>
<span class="sd">        :param None voltage_dependence: not supported</span>
<span class="sd">        :param float dendritic_delay_fraction: must be 1.0!</span>
<span class="sd">        :param float weight:</span>
<span class="sd">        :param delay: Use ``None`` to get the simulator default minimum delay.</span>
<span class="sd">        :type delay: float or None</span>
<span class="sd">        :param pad_to_length:</span>
<span class="sd">        :type pad_to_length: int or None</span>
<span class="sd">        :param bool backprop_delay:</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">timing_dependence</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">weight_dependence</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
                <span class="s2">&quot;Both timing_dependence and weight_dependence must be&quot;</span>
                <span class="s2">&quot;specified&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">voltage_dependence</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
                <span class="s2">&quot;Voltage dependence has not been implemented&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">__timing_dependence</span> <span class="o">=</span> <span class="n">timing_dependence</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">__weight_dependence</span> <span class="o">=</span> <span class="n">weight_dependence</span>
        <span class="c1"># move data from timing to weight dependence; that&#39;s where we need it</span>
        <span class="n">weight_dependence</span><span class="o">.</span><span class="n">set_a_plus_a_minus</span><span class="p">(</span>
            <span class="n">timing_dependence</span><span class="o">.</span><span class="n">A_plus</span><span class="p">,</span> <span class="n">timing_dependence</span><span class="o">.</span><span class="n">A_minus</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">__dendritic_delay_fraction</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">dendritic_delay_fraction</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">__change_requires_mapping</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">__pad_to_length</span> <span class="o">=</span> <span class="n">pad_to_length</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">__weight</span> <span class="o">=</span> <span class="n">weight</span>
        <span class="k">if</span> <span class="n">delay</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">delay</span> <span class="o">=</span> <span class="n">get_simulator</span><span class="p">()</span><span class="o">.</span><span class="n">min_delay</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">__delay</span> <span class="o">=</span> <span class="n">delay</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">__backprop_delay</span> <span class="o">=</span> <span class="n">backprop_delay</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">__neuromodulation</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">__dendritic_delay_fraction</span> <span class="o">!=</span> <span class="mf">1.0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;All delays must be dendritic!&quot;</span><span class="p">)</span>

<div class="viewcode-block" id="SynapseDynamicsSTDP.merge_neuromodulation"><a class="viewcode-back" href="../../../../../../spynnaker.pyNN.models.neuron.synapse_dynamics.html#spynnaker.pyNN.models.neuron.synapse_dynamics.SynapseDynamicsSTDP.merge_neuromodulation">[docs]</a>    <span class="k">def</span> <span class="nf">merge_neuromodulation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">neuromodulation</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">__neuromodulation</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">__neuromodulation</span> <span class="o">=</span> <span class="n">neuromodulation</span>
        <span class="k">elif</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">__neuromodulation</span><span class="o">.</span><span class="n">is_neuromodulation_same_as</span><span class="p">(</span>
                <span class="n">neuromodulation</span><span class="p">):</span>
            <span class="k">raise</span> <span class="n">SynapticConfigurationException</span><span class="p">(</span>
                <span class="s2">&quot;Neuromodulation must match exactly when using multiple&quot;</span>
                <span class="s2">&quot; edges to the same Population&quot;</span><span class="p">)</span></div>

<div class="viewcode-block" id="SynapseDynamicsSTDP.merge"><a class="viewcode-back" href="../../../../../../spynnaker.pyNN.models.neuron.synapse_dynamics.html#spynnaker.pyNN.models.neuron.synapse_dynamics.SynapseDynamicsSTDP.merge">[docs]</a>    <span class="nd">@overrides</span><span class="p">(</span><span class="n">AbstractPlasticSynapseDynamics</span><span class="o">.</span><span class="n">merge</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">merge</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">synapse_dynamics</span><span class="p">):</span>
        <span class="c1"># If dynamics is Neuromodulation, merge with other neuromodulation,</span>
        <span class="c1"># and then return ourselves, as neuromodulation can&#39;t be used by</span>
        <span class="c1"># itself</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">synapse_dynamics</span><span class="p">,</span> <span class="n">SynapseDynamicsNeuromodulation</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">merge_neuromodulation</span><span class="p">(</span><span class="n">synapse_dynamics</span><span class="p">)</span>
            <span class="k">return</span> <span class="bp">self</span>

        <span class="c1"># If dynamics is STDP, test if same as</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">synapse_dynamics</span><span class="p">,</span> <span class="n">SynapseDynamicsSTDP</span><span class="p">):</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_same_as</span><span class="p">(</span><span class="n">synapse_dynamics</span><span class="p">):</span>
                <span class="k">raise</span> <span class="n">SynapticConfigurationException</span><span class="p">(</span>
                    <span class="s2">&quot;Synapse dynamics must match exactly when using multiple&quot;</span>
                    <span class="s2">&quot; edges to the same population&quot;</span><span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">__neuromodulation</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">synapse_dynamics</span><span class="o">.</span><span class="n">merge_neuromodulation</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">__neuromodulation</span><span class="p">)</span>

            <span class="c1"># If STDP part matches, return the other, as it might also be</span>
            <span class="c1"># structural</span>
            <span class="k">return</span> <span class="n">synapse_dynamics</span>

        <span class="c1"># If dynamics is structural but not STDP (as here), merge</span>
        <span class="c1"># NOTE: Import here as otherwise we get a circular dependency</span>
        <span class="kn">from</span> <span class="nn">.synapse_dynamics_structural_stdp</span> <span class="kn">import</span> <span class="p">(</span>
            <span class="n">SynapseDynamicsStructuralSTDP</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">synapse_dynamics</span><span class="p">,</span> <span class="n">AbstractSynapseDynamicsStructural</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">SynapseDynamicsStructuralSTDP</span><span class="p">(</span>
                <span class="n">synapse_dynamics</span><span class="o">.</span><span class="n">partner_selection</span><span class="p">,</span> <span class="n">synapse_dynamics</span><span class="o">.</span><span class="n">formation</span><span class="p">,</span>
                <span class="n">synapse_dynamics</span><span class="o">.</span><span class="n">elimination</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">timing_dependence</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_dependence</span><span class="p">,</span>
                <span class="c1"># voltage dependence is not supported</span>
                <span class="kc">None</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dendritic_delay_fraction</span><span class="p">,</span>
                <span class="n">synapse_dynamics</span><span class="o">.</span><span class="n">f_rew</span><span class="p">,</span> <span class="n">synapse_dynamics</span><span class="o">.</span><span class="n">initial_weight</span><span class="p">,</span>
                <span class="n">synapse_dynamics</span><span class="o">.</span><span class="n">initial_delay</span><span class="p">,</span> <span class="n">synapse_dynamics</span><span class="o">.</span><span class="n">s_max</span><span class="p">,</span>
                <span class="n">synapse_dynamics</span><span class="o">.</span><span class="n">seed</span><span class="p">,</span>
                <span class="n">backprop_delay</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">backprop_delay</span><span class="p">)</span>

        <span class="c1"># Otherwise, it is static or neuromodulation, so return ourselves</span>
        <span class="k">return</span> <span class="bp">self</span></div>

    <span class="nd">@property</span>
    <span class="nd">@overrides</span><span class="p">(</span><span class="n">AbstractChangableAfterRun</span><span class="o">.</span><span class="n">requires_mapping</span><span class="p">,</span> <span class="n">extend_doc</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">requires_mapping</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; True if changes that have been made require that mapping be\</span>
<span class="sd">            performed.  Note that this should return True the first time it\</span>
<span class="sd">            is called, as the vertex must require mapping as it has been\</span>
<span class="sd">            created!</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">__change_requires_mapping</span>

<div class="viewcode-block" id="SynapseDynamicsSTDP.mark_no_changes"><a class="viewcode-back" href="../../../../../../spynnaker.pyNN.models.neuron.synapse_dynamics.html#spynnaker.pyNN.models.neuron.synapse_dynamics.SynapseDynamicsSTDP.mark_no_changes">[docs]</a>    <span class="nd">@overrides</span><span class="p">(</span><span class="n">AbstractChangableAfterRun</span><span class="o">.</span><span class="n">mark_no_changes</span><span class="p">,</span> <span class="n">extend_doc</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">mark_no_changes</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Marks the point after which changes are reported.  Immediately\</span>
<span class="sd">            after calling this method, requires_mapping should return False.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">__change_requires_mapping</span> <span class="o">=</span> <span class="kc">False</span></div>

<div class="viewcode-block" id="SynapseDynamicsSTDP.get_value"><a class="viewcode-back" href="../../../../../../spynnaker.pyNN.models.neuron.synapse_dynamics.html#spynnaker.pyNN.models.neuron.synapse_dynamics.SynapseDynamicsSTDP.get_value">[docs]</a>    <span class="nd">@overrides</span><span class="p">(</span><span class="n">AbstractSettable</span><span class="o">.</span><span class="n">get_value</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">get_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">obj</span> <span class="ow">in</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">__timing_dependence</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">__weight_dependence</span><span class="p">,</span> <span class="bp">self</span><span class="p">]:</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">key</span><span class="p">):</span>
                <span class="k">return</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">key</span><span class="p">)</span>
        <span class="k">raise</span> <span class="n">InvalidParameterType</span><span class="p">(</span>
            <span class="s2">&quot;Type </span><span class="si">{}</span><span class="s2"> does not have parameter </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">),</span> <span class="n">key</span><span class="p">))</span></div>

<div class="viewcode-block" id="SynapseDynamicsSTDP.set_value"><a class="viewcode-back" href="../../../../../../spynnaker.pyNN.models.neuron.synapse_dynamics.html#spynnaker.pyNN.models.neuron.synapse_dynamics.SynapseDynamicsSTDP.set_value">[docs]</a>    <span class="nd">@overrides</span><span class="p">(</span><span class="n">AbstractSettable</span><span class="o">.</span><span class="n">set_value</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">set_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">obj</span> <span class="ow">in</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">__timing_dependence</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">__weight_dependence</span><span class="p">,</span> <span class="bp">self</span><span class="p">]:</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">key</span><span class="p">):</span>
                <span class="nb">setattr</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">__change_requires_mapping</span> <span class="o">=</span> <span class="kc">True</span>
                <span class="k">return</span>
        <span class="k">raise</span> <span class="n">InvalidParameterType</span><span class="p">(</span>
            <span class="s2">&quot;Type </span><span class="si">{}</span><span class="s2"> does not have parameter </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">),</span> <span class="n">key</span><span class="p">))</span></div>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">weight_dependence</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        :rtype: AbstractTimingDependence</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">__weight_dependence</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">timing_dependence</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        :rtype: AbstractTimingDependence</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">__timing_dependence</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">dendritic_delay_fraction</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Settable.</span>

<span class="sd">        :rtype: float</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">__dendritic_delay_fraction</span>

    <span class="nd">@dendritic_delay_fraction</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">dendritic_delay_fraction</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">new_value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">__dendritic_delay_fraction</span> <span class="o">=</span> <span class="n">new_value</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">backprop_delay</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Settable.</span>

<span class="sd">        :rtype: bool</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">__backprop_delay</span>

    <span class="nd">@backprop_delay</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">backprop_delay</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">backprop_delay</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">__backprop_delay</span> <span class="o">=</span> <span class="nb">bool</span><span class="p">(</span><span class="n">backprop_delay</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">neuromodulation</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        :rtype: SynapseDynamicsNeuromodulation</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">__neuromodulation</span>

<div class="viewcode-block" id="SynapseDynamicsSTDP.is_same_as"><a class="viewcode-back" href="../../../../../../spynnaker.pyNN.models.neuron.synapse_dynamics.html#spynnaker.pyNN.models.neuron.synapse_dynamics.SynapseDynamicsSTDP.is_same_as">[docs]</a>    <span class="nd">@overrides</span><span class="p">(</span><span class="n">AbstractPlasticSynapseDynamics</span><span class="o">.</span><span class="n">is_same_as</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">is_same_as</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">synapse_dynamics</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">synapse_dynamics</span><span class="p">,</span> <span class="n">SynapseDynamicsSTDP</span><span class="p">):</span>
            <span class="k">return</span> <span class="kc">False</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">__timing_dependence</span><span class="o">.</span><span class="n">is_same_as</span><span class="p">(</span>
                <span class="n">synapse_dynamics</span><span class="o">.</span><span class="n">timing_dependence</span><span class="p">)</span> <span class="ow">and</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">__weight_dependence</span><span class="o">.</span><span class="n">is_same_as</span><span class="p">(</span>
                <span class="n">synapse_dynamics</span><span class="o">.</span><span class="n">weight_dependence</span><span class="p">)</span> <span class="ow">and</span>
            <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">__dendritic_delay_fraction</span> <span class="o">==</span>
             <span class="n">synapse_dynamics</span><span class="o">.</span><span class="n">dendritic_delay_fraction</span><span class="p">))</span></div>

<div class="viewcode-block" id="SynapseDynamicsSTDP.are_weights_signed"><a class="viewcode-back" href="../../../../../../spynnaker.pyNN.models.neuron.synapse_dynamics.html#spynnaker.pyNN.models.neuron.synapse_dynamics.SynapseDynamicsSTDP.are_weights_signed">[docs]</a>    <span class="k">def</span> <span class="nf">are_weights_signed</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        :rtype: bool</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="kc">False</span></div>

<div class="viewcode-block" id="SynapseDynamicsSTDP.get_vertex_executable_suffix"><a class="viewcode-back" href="../../../../../../spynnaker.pyNN.models.neuron.synapse_dynamics.html#spynnaker.pyNN.models.neuron.synapse_dynamics.SynapseDynamicsSTDP.get_vertex_executable_suffix">[docs]</a>    <span class="k">def</span> <span class="nf">get_vertex_executable_suffix</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        :rtype: str</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Get the suffix values for timing and weight dependence</span>
        <span class="n">timing_suffix</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">__timing_dependence</span><span class="o">.</span><span class="n">vertex_executable_suffix</span>
        <span class="n">weight_suffix</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">__weight_dependence</span><span class="o">.</span><span class="n">vertex_executable_suffix</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">__neuromodulation</span><span class="p">:</span>
            <span class="n">name</span> <span class="o">=</span> <span class="p">(</span>
                <span class="s2">&quot;_stdp_&quot;</span> <span class="o">+</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">__neuromodulation</span><span class="o">.</span><span class="n">get_vertex_executable_suffix</span><span class="p">())</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;_stdp_mad_&quot;</span>
        <span class="n">name</span> <span class="o">+=</span> <span class="n">timing_suffix</span> <span class="o">+</span> <span class="s2">&quot;_&quot;</span> <span class="o">+</span> <span class="n">weight_suffix</span>
        <span class="k">return</span> <span class="n">name</span></div>

<div class="viewcode-block" id="SynapseDynamicsSTDP.get_parameters_sdram_usage_in_bytes"><a class="viewcode-back" href="../../../../../../spynnaker.pyNN.models.neuron.synapse_dynamics.html#spynnaker.pyNN.models.neuron.synapse_dynamics.SynapseDynamicsSTDP.get_parameters_sdram_usage_in_bytes">[docs]</a>    <span class="k">def</span> <span class="nf">get_parameters_sdram_usage_in_bytes</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_neurons</span><span class="p">,</span> <span class="n">n_synapse_types</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        :param int n_neurons:</span>
<span class="sd">        :param int n_synapse_types:</span>
<span class="sd">        :rtype: int</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># 32-bits for back-prop delay</span>
        <span class="n">size</span> <span class="o">=</span> <span class="n">BYTES_PER_WORD</span>
        <span class="n">size</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">__timing_dependence</span><span class="o">.</span><span class="n">get_parameters_sdram_usage_in_bytes</span><span class="p">()</span>
        <span class="n">size</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">__weight_dependence</span><span class="o">.</span><span class="n">get_parameters_sdram_usage_in_bytes</span><span class="p">(</span>
            <span class="n">n_synapse_types</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">__timing_dependence</span><span class="o">.</span><span class="n">n_weight_terms</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">__neuromodulation</span><span class="p">:</span>
            <span class="n">size</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">__neuromodulation</span><span class="o">.</span><span class="n">get_parameters_sdram_usage_in_bytes</span><span class="p">(</span>
                <span class="n">n_neurons</span><span class="p">,</span> <span class="n">n_synapse_types</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">size</span></div>

<div class="viewcode-block" id="SynapseDynamicsSTDP.write_parameters"><a class="viewcode-back" href="../../../../../../spynnaker.pyNN.models.neuron.synapse_dynamics.html#spynnaker.pyNN.models.neuron.synapse_dynamics.SynapseDynamicsSTDP.write_parameters">[docs]</a>    <span class="nd">@overrides</span><span class="p">(</span><span class="n">AbstractPlasticSynapseDynamics</span><span class="o">.</span><span class="n">write_parameters</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">write_parameters</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span> <span class="n">spec</span><span class="p">,</span> <span class="n">region</span><span class="p">,</span> <span class="n">global_weight_scale</span><span class="p">,</span> <span class="n">synapse_weight_scales</span><span class="p">):</span>
        <span class="n">spec</span><span class="o">.</span><span class="n">comment</span><span class="p">(</span><span class="s2">&quot;Writing Plastic Parameters&quot;</span><span class="p">)</span>

        <span class="c1"># Switch focus to the region:</span>
        <span class="n">spec</span><span class="o">.</span><span class="n">switch_write_focus</span><span class="p">(</span><span class="n">region</span><span class="p">)</span>

        <span class="c1"># Whether to use back-prop delay</span>
        <span class="n">spec</span><span class="o">.</span><span class="n">write_value</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">__backprop_delay</span><span class="p">))</span>

        <span class="c1"># Write timing dependence parameters to region</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">__timing_dependence</span><span class="o">.</span><span class="n">write_parameters</span><span class="p">(</span>
            <span class="n">spec</span><span class="p">,</span> <span class="n">global_weight_scale</span><span class="p">,</span> <span class="n">synapse_weight_scales</span><span class="p">)</span>

        <span class="c1"># Write weight dependence information to region</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">__weight_dependence</span><span class="o">.</span><span class="n">write_parameters</span><span class="p">(</span>
            <span class="n">spec</span><span class="p">,</span> <span class="n">global_weight_scale</span><span class="p">,</span> <span class="n">synapse_weight_scales</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">__timing_dependence</span><span class="o">.</span><span class="n">n_weight_terms</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">__neuromodulation</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">__neuromodulation</span><span class="o">.</span><span class="n">write_parameters</span><span class="p">(</span>
                <span class="n">spec</span><span class="p">,</span> <span class="n">region</span><span class="p">,</span> <span class="n">global_weight_scale</span><span class="p">,</span> <span class="n">synapse_weight_scales</span><span class="p">)</span></div>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">_n_header_bytes</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        :rtype: int</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># The header contains a single timestamp and pre-trace</span>
        <span class="n">n_bytes</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">TIME_STAMP_BYTES</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">__timing_dependence</span><span class="o">.</span><span class="n">pre_trace_n_bytes</span><span class="p">)</span>

        <span class="c1"># The actual number of bytes is in a word-aligned struct, so work out</span>
        <span class="c1"># the number of bytes as a number of words</span>
        <span class="k">return</span> <span class="nb">int</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">n_bytes</span><span class="p">)</span> <span class="o">/</span> <span class="n">BYTES_PER_WORD</span><span class="p">))</span> <span class="o">*</span> <span class="n">BYTES_PER_WORD</span>

    <span class="k">def</span> <span class="nf">__get_n_connections</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_connections</span><span class="p">,</span> <span class="n">check_length_padded</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        :param int n_connections:</span>
<span class="sd">        :rtype: int</span>
<span class="sd">        :param bool check_length_padded:</span>
<span class="sd">        :rtype: bool</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">synapse_structure</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">__timing_dependence</span><span class="o">.</span><span class="n">synaptic_structure</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">__pad_to_length</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">check_length_padded</span><span class="p">:</span>
            <span class="n">n_connections</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">n_connections</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">__pad_to_length</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">n_connections</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="mi">0</span>
        <span class="c1"># 2 == two half words per word</span>
        <span class="n">fp_size_words</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">n_connections</span> <span class="o">//</span> <span class="mi">2</span> <span class="k">if</span> <span class="n">n_connections</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span>
            <span class="k">else</span> <span class="p">(</span><span class="n">n_connections</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">pp_size_bytes</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_n_header_bytes</span> <span class="o">+</span>
            <span class="p">(</span><span class="n">synapse_structure</span><span class="o">.</span><span class="n">get_n_half_words_per_connection</span><span class="p">()</span> <span class="o">*</span>
             <span class="n">BYTES_PER_SHORT</span> <span class="o">*</span> <span class="n">n_connections</span><span class="p">))</span>
        <span class="c1"># Neuromodulated synapses have the actual weight separately</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">__neuromodulation</span><span class="p">:</span>
            <span class="n">pp_size_bytes</span> <span class="o">+=</span> <span class="n">BYTES_PER_SHORT</span> <span class="o">*</span> <span class="n">n_connections</span>
        <span class="n">pp_size_words</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">pp_size_bytes</span><span class="p">)</span> <span class="o">/</span> <span class="n">BYTES_PER_WORD</span><span class="p">))</span>

        <span class="k">return</span> <span class="n">fp_size_words</span> <span class="o">+</span> <span class="n">pp_size_words</span>

<div class="viewcode-block" id="SynapseDynamicsSTDP.get_n_words_for_plastic_connections"><a class="viewcode-back" href="../../../../../../spynnaker.pyNN.models.neuron.synapse_dynamics.html#spynnaker.pyNN.models.neuron.synapse_dynamics.SynapseDynamicsSTDP.get_n_words_for_plastic_connections">[docs]</a>    <span class="k">def</span> <span class="nf">get_n_words_for_plastic_connections</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_connections</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        :param int n_connections:</span>
<span class="sd">        :rtype: int</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">__get_n_connections</span><span class="p">(</span><span class="n">n_connections</span><span class="p">)</span></div>

<div class="viewcode-block" id="SynapseDynamicsSTDP.get_plastic_synaptic_data"><a class="viewcode-back" href="../../../../../../spynnaker.pyNN.models.neuron.synapse_dynamics.html#spynnaker.pyNN.models.neuron.synapse_dynamics.SynapseDynamicsSTDP.get_plastic_synaptic_data">[docs]</a>    <span class="nd">@overrides</span><span class="p">(</span><span class="n">AbstractPlasticSynapseDynamics</span><span class="o">.</span><span class="n">get_plastic_synaptic_data</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">get_plastic_synaptic_data</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span> <span class="n">connections</span><span class="p">,</span> <span class="n">connection_row_indices</span><span class="p">,</span> <span class="n">n_rows</span><span class="p">,</span>
            <span class="n">post_vertex_slice</span><span class="p">,</span> <span class="n">n_synapse_types</span><span class="p">,</span> <span class="n">max_n_synapses</span><span class="p">):</span>
        <span class="c1"># pylint: disable=too-many-arguments</span>
        <span class="n">n_synapse_type_bits</span> <span class="o">=</span> <span class="n">get_n_bits</span><span class="p">(</span><span class="n">n_synapse_types</span><span class="p">)</span>
        <span class="n">n_neuron_id_bits</span> <span class="o">=</span> <span class="n">get_n_bits</span><span class="p">(</span><span class="n">post_vertex_slice</span><span class="o">.</span><span class="n">n_atoms</span><span class="p">)</span>
        <span class="n">neuron_id_mask</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="n">n_neuron_id_bits</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>

        <span class="c1"># Get the fixed data</span>
        <span class="n">fixed_plastic</span> <span class="o">=</span> <span class="p">(</span>
            <span class="p">(</span><span class="n">connections</span><span class="p">[</span><span class="s2">&quot;delay&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;uint16&quot;</span><span class="p">)</span> <span class="o">&lt;&lt;</span>
             <span class="p">(</span><span class="n">n_neuron_id_bits</span> <span class="o">+</span> <span class="n">n_synapse_type_bits</span><span class="p">))</span> <span class="o">|</span>
            <span class="p">(</span><span class="n">connections</span><span class="p">[</span><span class="s2">&quot;synapse_type&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;uint16&quot;</span><span class="p">)</span>
             <span class="o">&lt;&lt;</span> <span class="n">n_neuron_id_bits</span><span class="p">)</span> <span class="o">|</span>
            <span class="p">((</span><span class="n">connections</span><span class="p">[</span><span class="s2">&quot;target&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;uint16&quot;</span><span class="p">)</span> <span class="o">-</span>
              <span class="n">post_vertex_slice</span><span class="o">.</span><span class="n">lo_atom</span><span class="p">)</span> <span class="o">&amp;</span> <span class="n">neuron_id_mask</span><span class="p">))</span>
        <span class="n">fixed_plastic_rows</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convert_per_connection_data_to_rows</span><span class="p">(</span>
            <span class="n">connection_row_indices</span><span class="p">,</span> <span class="n">n_rows</span><span class="p">,</span>
            <span class="n">fixed_plastic</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;uint8&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)),</span>
            <span class="n">max_n_synapses</span><span class="p">)</span>
        <span class="n">fp_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_n_items</span><span class="p">(</span><span class="n">fixed_plastic_rows</span><span class="p">,</span> <span class="n">BYTES_PER_SHORT</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">__pad_to_length</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Pad the data</span>
            <span class="n">fixed_plastic_rows</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pad_row</span><span class="p">(</span>
                <span class="n">fixed_plastic_rows</span><span class="p">,</span> <span class="n">BYTES_PER_SHORT</span><span class="p">)</span>
        <span class="n">fp_data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_words</span><span class="p">(</span><span class="n">fixed_plastic_rows</span><span class="p">)</span>

        <span class="c1"># Get the plastic data by inserting the weight into the half-word</span>
        <span class="c1"># specified by the synapse structure</span>
        <span class="n">synapse_structure</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">__timing_dependence</span><span class="o">.</span><span class="n">synaptic_structure</span>
        <span class="n">n_half_words</span> <span class="o">=</span> <span class="n">synapse_structure</span><span class="o">.</span><span class="n">get_n_half_words_per_connection</span><span class="p">()</span>
        <span class="n">half_word</span> <span class="o">=</span> <span class="n">synapse_structure</span><span class="o">.</span><span class="n">get_weight_half_word</span><span class="p">()</span>
        <span class="c1"># If neuromodulation, the real weight comes first</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">__neuromodulation</span><span class="p">:</span>
            <span class="n">n_half_words</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">half_word</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">plastic_plastic</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
            <span class="nb">len</span><span class="p">(</span><span class="n">connections</span><span class="p">)</span> <span class="o">*</span> <span class="n">n_half_words</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;uint16&quot;</span><span class="p">)</span>
        <span class="n">plastic_plastic</span><span class="p">[</span><span class="n">half_word</span><span class="p">::</span><span class="n">n_half_words</span><span class="p">]</span> <span class="o">=</span> \
            <span class="n">numpy</span><span class="o">.</span><span class="n">rint</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">connections</span><span class="p">[</span><span class="s2">&quot;weight&quot;</span><span class="p">]))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;uint16&quot;</span><span class="p">)</span>

        <span class="c1"># Convert the plastic data into groups of bytes per connection and</span>
        <span class="c1"># then into rows</span>
        <span class="n">plastic_plastic</span> <span class="o">=</span> <span class="n">plastic_plastic</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;uint8&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
            <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_half_words</span> <span class="o">*</span> <span class="n">BYTES_PER_SHORT</span><span class="p">))</span>
        <span class="n">plastic_plastic_row_data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convert_per_connection_data_to_rows</span><span class="p">(</span>
            <span class="n">connection_row_indices</span><span class="p">,</span> <span class="n">n_rows</span><span class="p">,</span> <span class="n">plastic_plastic</span><span class="p">,</span> <span class="n">max_n_synapses</span><span class="p">)</span>

        <span class="c1"># pp_size = fp_size in words =&gt; fp_size * no_bytes / 4 (bytes)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">__pad_to_length</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Pad the data</span>
            <span class="n">plastic_plastic_row_data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pad_row</span><span class="p">(</span>
                <span class="n">plastic_plastic_row_data</span><span class="p">,</span> <span class="n">n_half_words</span> <span class="o">*</span> <span class="n">BYTES_PER_SHORT</span><span class="p">)</span>
        <span class="n">plastic_headers</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
            <span class="p">(</span><span class="n">n_rows</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_header_bytes</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;uint8&quot;</span><span class="p">)</span>
        <span class="n">plastic_plastic_rows</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">numpy</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span>
                <span class="n">plastic_headers</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">plastic_plastic_row_data</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_rows</span><span class="p">)]</span>
        <span class="n">pp_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_n_items</span><span class="p">(</span><span class="n">plastic_plastic_rows</span><span class="p">,</span> <span class="n">BYTES_PER_WORD</span><span class="p">)</span>
        <span class="n">pp_data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_words</span><span class="p">(</span><span class="n">plastic_plastic_rows</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">fp_data</span><span class="p">,</span> <span class="n">pp_data</span><span class="p">,</span> <span class="n">fp_size</span><span class="p">,</span> <span class="n">pp_size</span></div>

    <span class="k">def</span> <span class="nf">_pad_row</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">rows</span><span class="p">,</span> <span class="n">no_bytes_per_connection</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        :param list(~numpy.ndarray) rows:</span>
<span class="sd">        :param int no_bytes_per_connection:</span>
<span class="sd">        :rtype: list(~numpy.ndarray)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Row elements are (individual) bytes</span>
        <span class="k">return</span> <span class="p">[</span>
            <span class="n">numpy</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span>
                <span class="n">row</span><span class="p">,</span> <span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
                    <span class="n">numpy</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span>
                        <span class="p">(</span><span class="n">no_bytes_per_connection</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">__pad_to_length</span> <span class="o">-</span>
                         <span class="n">row</span><span class="o">.</span><span class="n">size</span><span class="p">),</span>
                        <span class="mi">0</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;uint8&quot;</span><span class="p">))</span>
                <span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;uint8&quot;</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">rows</span><span class="p">]</span>

<div class="viewcode-block" id="SynapseDynamicsSTDP.get_n_plastic_plastic_words_per_row"><a class="viewcode-back" href="../../../../../../spynnaker.pyNN.models.neuron.synapse_dynamics.html#spynnaker.pyNN.models.neuron.synapse_dynamics.SynapseDynamicsSTDP.get_n_plastic_plastic_words_per_row">[docs]</a>    <span class="nd">@overrides</span><span class="p">(</span>
        <span class="n">AbstractPlasticSynapseDynamics</span><span class="o">.</span><span class="n">get_n_plastic_plastic_words_per_row</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">get_n_plastic_plastic_words_per_row</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pp_size</span><span class="p">):</span>
        <span class="c1"># pp_size is in words, so return</span>
        <span class="k">return</span> <span class="n">pp_size</span></div>

<div class="viewcode-block" id="SynapseDynamicsSTDP.get_n_fixed_plastic_words_per_row"><a class="viewcode-back" href="../../../../../../spynnaker.pyNN.models.neuron.synapse_dynamics.html#spynnaker.pyNN.models.neuron.synapse_dynamics.SynapseDynamicsSTDP.get_n_fixed_plastic_words_per_row">[docs]</a>    <span class="nd">@overrides</span><span class="p">(</span>
        <span class="n">AbstractPlasticSynapseDynamics</span><span class="o">.</span><span class="n">get_n_fixed_plastic_words_per_row</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">get_n_fixed_plastic_words_per_row</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fp_size</span><span class="p">):</span>
        <span class="c1"># fp_size is in half-words</span>
        <span class="k">return</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">fp_size</span> <span class="o">/</span> <span class="mf">2.0</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;uint32&quot;</span><span class="p">)</span></div>

<div class="viewcode-block" id="SynapseDynamicsSTDP.get_n_synapses_in_rows"><a class="viewcode-back" href="../../../../../../spynnaker.pyNN.models.neuron.synapse_dynamics.html#spynnaker.pyNN.models.neuron.synapse_dynamics.SynapseDynamicsSTDP.get_n_synapses_in_rows">[docs]</a>    <span class="nd">@overrides</span><span class="p">(</span><span class="n">AbstractPlasticSynapseDynamics</span><span class="o">.</span><span class="n">get_n_synapses_in_rows</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">get_n_synapses_in_rows</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pp_size</span><span class="p">,</span> <span class="n">fp_size</span><span class="p">):</span>
        <span class="c1"># Each fixed-plastic synapse is a half-word and fp_size is in half</span>
        <span class="c1"># words so just return it</span>
        <span class="k">return</span> <span class="n">fp_size</span></div>

<div class="viewcode-block" id="SynapseDynamicsSTDP.read_plastic_synaptic_data"><a class="viewcode-back" href="../../../../../../spynnaker.pyNN.models.neuron.synapse_dynamics.html#spynnaker.pyNN.models.neuron.synapse_dynamics.SynapseDynamicsSTDP.read_plastic_synaptic_data">[docs]</a>    <span class="nd">@overrides</span><span class="p">(</span><span class="n">AbstractPlasticSynapseDynamics</span><span class="o">.</span><span class="n">read_plastic_synaptic_data</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">read_plastic_synaptic_data</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span> <span class="n">post_vertex_slice</span><span class="p">,</span> <span class="n">n_synapse_types</span><span class="p">,</span> <span class="n">pp_size</span><span class="p">,</span> <span class="n">pp_data</span><span class="p">,</span>
            <span class="n">fp_size</span><span class="p">,</span> <span class="n">fp_data</span><span class="p">):</span>
        <span class="c1"># pylint: disable=too-many-arguments</span>
        <span class="n">n_rows</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">fp_size</span><span class="p">)</span>

        <span class="n">n_synapse_type_bits</span> <span class="o">=</span> <span class="n">get_n_bits</span><span class="p">(</span><span class="n">n_synapse_types</span><span class="p">)</span>
        <span class="n">n_neuron_id_bits</span> <span class="o">=</span> <span class="n">get_n_bits</span><span class="p">(</span><span class="n">post_vertex_slice</span><span class="o">.</span><span class="n">n_atoms</span><span class="p">)</span>
        <span class="n">neuron_id_mask</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="n">n_neuron_id_bits</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>

        <span class="n">data_fixed</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span>
            <span class="n">fp_data</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;uint16&quot;</span><span class="p">)[</span><span class="mi">0</span><span class="p">:</span><span class="n">fp_size</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_rows</span><span class="p">)])</span>
        <span class="n">pp_without_headers</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">row</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;uint8&quot;</span><span class="p">)[</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_header_bytes</span><span class="p">:]</span> <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">pp_data</span><span class="p">]</span>
        <span class="n">synapse_structure</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">__timing_dependence</span><span class="o">.</span><span class="n">synaptic_structure</span>
        <span class="n">n_half_words</span> <span class="o">=</span> <span class="n">synapse_structure</span><span class="o">.</span><span class="n">get_n_half_words_per_connection</span><span class="p">()</span>
        <span class="n">half_word</span> <span class="o">=</span> <span class="n">synapse_structure</span><span class="o">.</span><span class="n">get_weight_half_word</span><span class="p">()</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">__neuromodulation</span><span class="p">:</span>
            <span class="n">n_half_words</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">half_word</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">pp_half_words</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span>
            <span class="n">pp</span><span class="p">[:</span><span class="n">size</span> <span class="o">*</span> <span class="n">n_half_words</span> <span class="o">*</span> <span class="n">BYTES_PER_SHORT</span><span class="p">]</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="s2">&quot;uint16&quot;</span><span class="p">)[</span>
                <span class="n">half_word</span><span class="p">::</span><span class="n">n_half_words</span><span class="p">]</span>
            <span class="k">for</span> <span class="n">pp</span><span class="p">,</span> <span class="n">size</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">pp_without_headers</span><span class="p">,</span> <span class="n">fp_size</span><span class="p">)])</span>

        <span class="n">connections</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
            <span class="n">data_fixed</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">NUMPY_CONNECTORS_DTYPE</span><span class="p">)</span>
        <span class="n">connections</span><span class="p">[</span><span class="s2">&quot;source&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span>
            <span class="p">[</span><span class="n">numpy</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">fp_size</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">fp_size</span><span class="p">))])</span>
        <span class="n">connections</span><span class="p">[</span><span class="s2">&quot;target&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
            <span class="p">(</span><span class="n">data_fixed</span> <span class="o">&amp;</span> <span class="n">neuron_id_mask</span><span class="p">)</span> <span class="o">+</span> <span class="n">post_vertex_slice</span><span class="o">.</span><span class="n">lo_atom</span><span class="p">)</span>
        <span class="n">connections</span><span class="p">[</span><span class="s2">&quot;weight&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pp_half_words</span>
        <span class="n">connections</span><span class="p">[</span><span class="s2">&quot;delay&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">data_fixed</span> <span class="o">&gt;&gt;</span> <span class="p">(</span>
            <span class="n">n_neuron_id_bits</span> <span class="o">+</span> <span class="n">n_synapse_type_bits</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">connections</span></div>

<div class="viewcode-block" id="SynapseDynamicsSTDP.get_weight_mean"><a class="viewcode-back" href="../../../../../../spynnaker.pyNN.models.neuron.synapse_dynamics.html#spynnaker.pyNN.models.neuron.synapse_dynamics.SynapseDynamicsSTDP.get_weight_mean">[docs]</a>    <span class="nd">@overrides</span><span class="p">(</span><span class="n">AbstractPlasticSynapseDynamics</span><span class="o">.</span><span class="n">get_weight_mean</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">get_weight_mean</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">connector</span><span class="p">,</span> <span class="n">synapse_info</span><span class="p">):</span>
        <span class="c1"># Because the weights could all be changed to the maximum, the mean</span>
        <span class="c1"># has to be given as the maximum for scaling</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_weight_maximum</span><span class="p">(</span><span class="n">connector</span><span class="p">,</span> <span class="n">synapse_info</span><span class="p">)</span></div>

<div class="viewcode-block" id="SynapseDynamicsSTDP.get_weight_variance"><a class="viewcode-back" href="../../../../../../spynnaker.pyNN.models.neuron.synapse_dynamics.html#spynnaker.pyNN.models.neuron.synapse_dynamics.SynapseDynamicsSTDP.get_weight_variance">[docs]</a>    <span class="nd">@overrides</span><span class="p">(</span><span class="n">AbstractPlasticSynapseDynamics</span><span class="o">.</span><span class="n">get_weight_variance</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">get_weight_variance</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">connector</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">synapse_info</span><span class="p">):</span>
        <span class="c1"># Because the weights could all be changed to the maximum, the variance</span>
        <span class="c1"># has to be given as no variance</span>
        <span class="k">return</span> <span class="mf">0.0</span></div>

<div class="viewcode-block" id="SynapseDynamicsSTDP.get_weight_maximum"><a class="viewcode-back" href="../../../../../../spynnaker.pyNN.models.neuron.synapse_dynamics.html#spynnaker.pyNN.models.neuron.synapse_dynamics.SynapseDynamicsSTDP.get_weight_maximum">[docs]</a>    <span class="nd">@overrides</span><span class="p">(</span><span class="n">AbstractPlasticSynapseDynamics</span><span class="o">.</span><span class="n">get_weight_maximum</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">get_weight_maximum</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">connector</span><span class="p">,</span> <span class="n">synapse_info</span><span class="p">):</span>
        <span class="n">w_max</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">get_weight_maximum</span><span class="p">(</span><span class="n">connector</span><span class="p">,</span> <span class="n">synapse_info</span><span class="p">)</span>
        <span class="c1"># The maximum weight is the largest that it could be set to from</span>
        <span class="c1"># the weight dependence</span>
        <span class="k">return</span> <span class="nb">max</span><span class="p">(</span><span class="n">w_max</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">__weight_dependence</span><span class="o">.</span><span class="n">weight_maximum</span><span class="p">)</span></div>

<div class="viewcode-block" id="SynapseDynamicsSTDP.get_parameter_names"><a class="viewcode-back" href="../../../../../../spynnaker.pyNN.models.neuron.synapse_dynamics.html#spynnaker.pyNN.models.neuron.synapse_dynamics.SynapseDynamicsSTDP.get_parameter_names">[docs]</a>    <span class="nd">@overrides</span><span class="p">(</span><span class="n">AbstractPlasticSynapseDynamics</span><span class="o">.</span><span class="n">get_parameter_names</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">get_parameter_names</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">names</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;weight&#39;</span><span class="p">,</span> <span class="s1">&#39;delay&#39;</span><span class="p">]</span>
        <span class="n">names</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">__timing_dependence</span><span class="o">.</span><span class="n">get_parameter_names</span><span class="p">())</span>
        <span class="n">names</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">__weight_dependence</span><span class="o">.</span><span class="n">get_parameter_names</span><span class="p">())</span>
        <span class="k">return</span> <span class="n">names</span></div>

<div class="viewcode-block" id="SynapseDynamicsSTDP.get_max_synapses"><a class="viewcode-back" href="../../../../../../spynnaker.pyNN.models.neuron.synapse_dynamics.html#spynnaker.pyNN.models.neuron.synapse_dynamics.SynapseDynamicsSTDP.get_max_synapses">[docs]</a>    <span class="nd">@overrides</span><span class="p">(</span><span class="n">AbstractPlasticSynapseDynamics</span><span class="o">.</span><span class="n">get_max_synapses</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">get_max_synapses</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_words</span><span class="p">):</span>

        <span class="c1"># Subtract the header size that will always exist</span>
        <span class="n">n_header_words</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_header_bytes</span> <span class="o">//</span> <span class="n">BYTES_PER_WORD</span>
        <span class="n">n_words_space</span> <span class="o">=</span> <span class="n">n_words</span> <span class="o">-</span> <span class="n">n_header_words</span>

        <span class="c1"># Get plastic plastic size per connection</span>
        <span class="n">synapse_structure</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">__timing_dependence</span><span class="o">.</span><span class="n">synaptic_structure</span>
        <span class="n">bytes_per_pp</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">synapse_structure</span><span class="o">.</span><span class="n">get_n_half_words_per_connection</span><span class="p">()</span> <span class="o">*</span>
            <span class="n">BYTES_PER_SHORT</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">__neuromodulation</span><span class="p">:</span>
            <span class="n">bytes_per_pp</span> <span class="o">+=</span> <span class="n">BYTES_PER_SHORT</span>

        <span class="c1"># The fixed plastic size per connection is 2 bytes</span>
        <span class="n">bytes_per_fp</span> <span class="o">=</span> <span class="n">BYTES_PER_SHORT</span>

        <span class="c1"># Maximum possible connections, ignoring word alignment</span>
        <span class="n">n_connections</span> <span class="o">=</span> <span class="p">(</span><span class="n">n_words_space</span> <span class="o">*</span> <span class="n">BYTES_PER_WORD</span><span class="p">)</span> <span class="o">//</span> <span class="p">(</span>
            <span class="n">bytes_per_pp</span> <span class="o">+</span> <span class="n">bytes_per_fp</span><span class="p">)</span>

        <span class="n">check_length_padded</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="c1"># Reduce until correct</span>
        <span class="k">while</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">__get_n_connections</span><span class="p">(</span><span class="n">n_connections</span><span class="p">,</span> <span class="n">check_length_padded</span><span class="p">)</span> <span class="o">&gt;</span>
               <span class="n">n_words</span><span class="p">):</span>
            <span class="n">n_connections</span> <span class="o">-=</span> <span class="mi">1</span>

        <span class="k">return</span> <span class="n">n_connections</span></div>

    <span class="nd">@property</span>
    <span class="nd">@overrides</span><span class="p">(</span><span class="n">AbstractGenerateOnMachine</span><span class="o">.</span><span class="n">gen_matrix_id</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">gen_matrix_id</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">MatrixGeneratorID</span><span class="o">.</span><span class="n">STDP_MATRIX</span><span class="o">.</span><span class="n">value</span>

    <span class="nd">@property</span>
    <span class="nd">@overrides</span><span class="p">(</span><span class="n">AbstractGenerateOnMachine</span><span class="o">.</span><span class="n">gen_matrix_params</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">gen_matrix_params</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">synapse_struct</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">__timing_dependence</span><span class="o">.</span><span class="n">synaptic_structure</span>
        <span class="n">n_half_words</span> <span class="o">=</span> <span class="n">synapse_struct</span><span class="o">.</span><span class="n">get_n_half_words_per_connection</span><span class="p">()</span>
        <span class="n">half_word</span> <span class="o">=</span> <span class="n">synapse_struct</span><span class="o">.</span><span class="n">get_weight_half_word</span><span class="p">()</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">__neuromodulation</span><span class="p">:</span>
            <span class="n">n_half_words</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">half_word</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">return</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
            <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_header_bytes</span> <span class="o">//</span> <span class="n">BYTES_PER_SHORT</span><span class="p">,</span> <span class="n">n_half_words</span><span class="p">,</span> <span class="n">half_word</span><span class="p">],</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">numpy</span><span class="o">.</span><span class="n">uint32</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="nd">@overrides</span><span class="p">(</span><span class="n">AbstractGenerateOnMachine</span><span class="o">.</span>
               <span class="n">gen_matrix_params_size_in_bytes</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">gen_matrix_params_size_in_bytes</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">BYTES_PER_WORD</span>

    <span class="nd">@property</span>
    <span class="nd">@overrides</span><span class="p">(</span><span class="n">AbstractPlasticSynapseDynamics</span><span class="o">.</span><span class="n">changes_during_run</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">changes_during_run</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="kc">True</span>

    <span class="nd">@property</span>
    <span class="nd">@overrides</span><span class="p">(</span><span class="n">AbstractPlasticSynapseDynamics</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">weight</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">__weight</span>

    <span class="nd">@property</span>
    <span class="nd">@overrides</span><span class="p">(</span><span class="n">AbstractPlasticSynapseDynamics</span><span class="o">.</span><span class="n">delay</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">delay</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">__delay</span>

<div class="viewcode-block" id="SynapseDynamicsSTDP.set_delay"><a class="viewcode-back" href="../../../../../../spynnaker.pyNN.models.neuron.synapse_dynamics.html#spynnaker.pyNN.models.neuron.synapse_dynamics.SynapseDynamicsSTDP.set_delay">[docs]</a>    <span class="nd">@overrides</span><span class="p">(</span><span class="n">AbstractPlasticSynapseDynamics</span><span class="o">.</span><span class="n">set_delay</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">set_delay</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">delay</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">__delay</span> <span class="o">=</span> <span class="n">delay</span></div>

    <span class="nd">@property</span>
    <span class="nd">@overrides</span><span class="p">(</span><span class="n">AbstractPlasticSynapseDynamics</span><span class="o">.</span><span class="n">pad_to_length</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">pad_to_length</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">__pad_to_length</span>
</pre></div>

            <div class="clearer"></div>
          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../../../../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../../../../../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="../../../../../../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="nav-item nav-item-0"><a href="../../../../../../index.html">sPyNNaker 6.0.1 documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="../../../../../index.html" >Module code</a> &#187;</li>
          <li class="nav-item nav-item-2"><a href="../../../../pyNN.html" >spynnaker.pyNN</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">spynnaker.pyNN.models.neuron.synapse_dynamics.synapse_dynamics_stdp</a></li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 2014-2021.
      Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 5.0.1.
    </div>
  </body>
</html>